{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\17013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import wikipedia as wiki\n",
    "import spacy\n",
    "import yake\n",
    "# https://towardsdatascience.com/visualizing-networks-in-python-d70f4cbeb259\n",
    "# https://hub.gke2.mybinder.org/user/westhealth-pyvis-fc5gtj65/notebooks/notebooks/example.ipynb#\n",
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import wikipediaapi\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters for packages\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "language = \"en\"\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "max_ngram_size = 4\n",
    "deduplication_threshold = 0.3\n",
    "numOfKeywords = 10\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language,\n",
    "                                            n=max_ngram_size,\n",
    "                                            dedupLim=deduplication_threshold,\n",
    "                                            top=numOfKeywords,\n",
    "                                            features=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_named_entities(text):\n",
    "    # https://spacy.io/usage/linguistic-features#named-entities\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(x for x in text)\n",
    "        \n",
    "    doc = nlp(text)\n",
    "    ents = []\n",
    "    for ent in doc.ents:\n",
    "        ents.append([ent.text, ent.start_char, ent.end_char, ent.label_])\n",
    "\n",
    "    return ents\n",
    "\n",
    "def get_wiki(topic):\n",
    "    # https://pypi.org/project/wikipedia/\n",
    "    search = wiki.search(topic, results= 50, suggestion=True)\n",
    "        \n",
    "    print(\"\\n--WIKIPEDIA PAGE {}--\\n\".format(search[0][0]))\n",
    "    \n",
    "    page = wiki.WikipediaPage(search[0][0])\n",
    "    print(page)\n",
    "    \n",
    "    categories = page.categories\n",
    "    sections = page.sections\n",
    "    summary = page.summary\n",
    "    \n",
    "    text = wiki.summary(search[0][0])\n",
    "    suggest = wiki.suggest(topic)\n",
    "    \n",
    "    \n",
    "\n",
    "    return (text, search, suggest, categories, sections, summary)\n",
    "\n",
    "\n",
    "def yake_keywords(text):\n",
    "    # https://towardsdatascience.com/keyword-extraction-process-in-python-with-natural-language-processing-nlp-d769a9069d5c\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter keyword: Disney\n"
     ]
    }
   ],
   "source": [
    "g = Network(height='800px',width='1200px')\n",
    "\n",
    "keyword = input(\"enter keyword: \")\n",
    "\n",
    "'''\n",
    "text, search, suggest, categories, sections, summary = get_wiki(keyword)\n",
    "\n",
    "yake_kw = yake_keywords(text)\n",
    "ents = extract_named_entities(text)\n",
    "'''\n",
    "\n",
    "g.add_nodes([keyword], value=[400],\n",
    "                         title=[keyword],\n",
    "                         x=[21.4],\n",
    "                         y=[100.2],\n",
    "                         label=[keyword],\n",
    "                         color=['#00ff1e'])\n",
    "\n",
    "search = wiki.search(keyword, results=15)\n",
    "\n",
    "for kw in search:   \n",
    "    g.add_node(kw, label = kw, color = 'red' )\n",
    "    g.add_edge(keyword, kw)\n",
    "    \n",
    "    s_search = wiki.search(kw, results=5)\n",
    "    \n",
    "    for s_kw in s_search:\n",
    "            g.add_node(s_kw, label = s_kw, color = 'yellow' )\n",
    "            g.add_edge(kw, s_kw)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "def add_subject_graph(graph, keyword, results, color):\n",
    "    search = wiki.search(keyword, results)\n",
    "    \n",
    "    for kw in search:   \n",
    "        graph.add_node(kw, label = kw, color = color)\n",
    "        graph.add_edge(keyword, kw)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "g.toggle_physics(False)\n",
    "g.show_buttons(filter_=['physics'])\n",
    "g.show(\"example.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
